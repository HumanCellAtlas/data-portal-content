---
path: "/learn/userguides/data-lifecycle"
date: "2018-05-03"
title: "Data Lifecycle"
subTitle: "Vestibulum id ligula porta felis euismod semper. Donec odio dui."
---

## Data Lifecycle

### Introduction ###

The Human Cell Atlas (HCA) Data Coordination Platform (DCP) is built to ingest, organize, process and provide terabytes of single cell data generated by researchers around the world using multiple types of data generation protocols.

Here we describe the general process of data flow through the components of the platform. Please see the guides in the *Learn* section for more detailed information. 

### Data Ingest ###

Data flow begins with the ingestion of raw experimental data and its associated metadata. Currently this ingest process is supported by HCA data wranglers, an interactive UI, and programmatic access. As the project grows we will automate and simplify the process as much as possible. 

Following ingest, the data and metadata are validated. Data files are checked to make sure they are in the correct file format and are not empty or corrupt. Metadata is validated against a schema to make sure that required information has been supplied using the appropriate controlled language. Once validation is complete, data and metadata are automatically stored in the HCA DCP Data Store.

### Data Store ###
The Data Store is a cloud-native space for storage that leverages multiple cloud platforms. Currently, all data in the Data Store is stored in Amazon Web Services (AWS) and synchronized with Google Cloud Platform (GCP), ensuring that the data is accessible in each environment. Once data is stored in the Data Store it is available to anyone.

### Data Processing Pipelines ###

When raw data moves into the Data Store a notification is triggered; it is received by the Data Processing Pipeline Service, indicating data is available for processing. If a processing pipeline specific for that data type exists, the Service activates a series of three sub workflows to 1) obtain the data file(s) from the Data Store, 2) run the data through the appropriate pipeline, producing new files of analysis results, and 3) submit the new results back to the Ingest Service to be stored in the Data Store.

For example, for data generated using the Smart-seq2 methodology, data processing pipeline outputs include gene alignment, transcript quantification, and quality control assessments. Once ingested, the resulting data files are validated (similar to raw data) and moved to the Data Store.



### Accessing Data ###
Access to the Data Store is supported with REST API (and associated CLI) using the Data Store's Consumer API. In addition, we have developed a Data Browser that enables extensive browsing of the data through this Data Portal. Data will also be accessible through tools and portals developed by the community.

* All processing pipelines have been approved by the HCA Analysis Working Group.
